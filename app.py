import streamlit as st
from langchain_openai import ChatOpenAI
from config import LLM_PROVIDER, OPENAI_API_KEY
from embedding import get_embedding_model
from postgresql import get_pg_engine
from sqlalchemy import text

# åˆå§‹åŒ– LLM
def get_llm():
    if LLM_PROVIDER == "openai":
        return ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0,
            api_key=OPENAI_API_KEY,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            streaming=True,
        )
    elif LLM_PROVIDER == "ollama":
        return ChatOpenAI(
            model="llama3.1",
            openai_api_key="ollama",
            openai_api_base="http://10.96.196.63:11434/v1/",
            streaming=True,
        )

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_model = get_embedding_model()
# åˆå§‹åŒ– PostgreSQL
pg_engine = get_pg_engine()


def search_relevant_docs(query_text, top_k=3):
    """ åœ¨è³‡æ–™åº«ä¸­æª¢ç´¢èˆ‡ query_text æœ€ç›¸ä¼¼çš„æ–‡ä»¶ """
    query_embedding = embedding_model.embed_query(query_text)

    table_name = "wifi_knowledge_embedding_bge"  # è‹¥ä½¿ç”¨ OpenAIï¼Œæ”¹ç‚º `wifi_knowledge_embedding_openai`
    
    sql = f"""
    SELECT 
        document_name, 
        original_text, 
        file_path, 
        embedding <=> :query_embedding AS similarity
    FROM {table_name}
    ORDER BY similarity ASC
    LIMIT :top_k;
    """

    with pg_engine.connect() as connection:
        result = connection.execute(text(sql), {"query_embedding": query_embedding, "top_k": top_k})
        docs = result.fetchall()

    return docs

def format_context(docs):
    """ æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡æª”å…§å®¹ """
    context = "\n\n".join([f"ğŸ“„ **æª”æ¡ˆ**: {doc[0]}\nğŸ” **å…§å®¹**: {doc[1][:200]}...\nğŸ“‚ **ä¾†æº**: {doc[2]}" for doc in docs])
    return f"### ğŸ” ç›¸é—œæ–‡ä»¶å…§å®¹:\n\n{context}"

def answer_question(query):
    """ é€é RAG æª¢ç´¢è³‡æ–™ä¸¦è®“ LLM å›ç­” """
    docs = search_relevant_docs(query)
    context = format_context(docs)

    messages = [
        ("system", "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜ç®¡ç†åŠ©ç†ï¼Œè«‹æ ¹æ“šæä¾›çš„å…§å®¹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚"),
        ("system", context),
        ("human", query),
    ]

    llm = get_llm()
    response = ""
    for chunk in llm.stream(messages):
        response += chunk.content
        yield chunk.content  # é€æ­¥å›å‚³çµ¦ Streamlit å‰ç«¯










# ---------- Streamlit ä»‹é¢ ----------
st.set_page_config(page_title="çŸ¥è­˜ç®¡ç†ç³»çµ± RAG", page_icon="ğŸ“š", layout="wide")

st.title("ğŸ“š çŸ¥è­˜ç®¡ç†ç³»çµ± (RAG)")
st.write("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼Œç³»çµ±å°‡å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œå…§å®¹ä¸¦å›ç­”æ‚¨çš„å•é¡Œã€‚")

# ä½¿ç”¨è€…è¼¸å…¥
user_input = st.text_area("ğŸ’¬ è¼¸å…¥æ‚¨çš„å•é¡Œ:", height=150)

# é€å‡ºæŸ¥è©¢
if st.button("ğŸ” æŸ¥è©¢"):
    if user_input.strip():
        with st.spinner("ğŸ“– æ­£åœ¨æª¢ç´¢çŸ¥è­˜åº«ä¸¦ç”Ÿæˆå›ç­”..."):
            # é¡¯ç¤ºæª¢ç´¢çµæœ
            docs = search_relevant_docs(user_input)
            if docs:
                st.markdown(format_context(docs))
            else:
                st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„çŸ¥è­˜ã€‚")

            # ç”Ÿæˆç­”æ¡ˆ
            st.markdown("### ğŸ¤– AI å›ç­”:")
            response_area = st.empty()  # å»ºç«‹ä¸€å€‹å€åŸŸä¾†é¡¯ç¤ºå›æ‡‰
            response_text = ""

            for chunk in answer_question(user_input):
                response_text += chunk
                response_area.markdown(response_text)
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥å•é¡Œå¾Œå†æŸ¥è©¢ã€‚")
