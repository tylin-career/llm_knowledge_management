from langchain_openai import ChatOpenAI
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from main import retrieve_similar_chunks
from langchain_core.runnables.history import RunnableWithMessageHistory
import streamlit as st
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory
# from langchain_community.memory import ConversationBufferWindowMemory
from config import LLM_PROVIDER, OPENAI_API_KEY
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage


st.set_page_config(page_title='Streamlit çŸ¥è­˜ç®¡ç†å°è©±ç³»çµ±')
st.title("ğŸ’¬ Chatbot")
st.caption("ğŸš€ ASUS Knowledge Management Simulation Powered by NPSPO")

# åŠ å…¥è‡ªè¨‚ CSSï¼Œè®“ä¸‹æ‹‰é¸å–®å±•é–‹æ™‚æœ‰å‹•ç•«
st.markdown(
    """
    <style>
        /* è®“ selectbox æœ¬èº«æ»‘é¼ æ‡¸åœæ™‚æœ‰ç‰¹æ•ˆ */
        div[data-baseweb="select"] {
            transition: all 0.3s ease-in-out;
        }

        div[data-baseweb="select"]:hover {
            background-color: #f0f0f0 !important;
            border-radius: 8px;
        }

        /* è¨­å®šä¸‹æ‹‰é¸å–®æœ¬é«” */
        div[role="listbox"] {
            animation: fadeIn 0.3s ease-in-out;
        }

        /* å®šç¾©å‹•ç•« */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """,
    unsafe_allow_html=True
)


with st.sidebar:
    st.title("Navigation and Settings")
    model = st.selectbox(
        # 'Model', 'gpt-3.5-turbo'
        'Model', ['llama3.1', 'gpt-3.5-turbo']
    )
    openai_api_key = st.text_input(
        # 'OpenAI API Key', value = OPENAI_API_KEY, type = 'password'
        'OpenAI API Key', value = 'ollama', type = 'password'
    )
    openai_api_base = st.text_input(
        # 'OpenAI API Base', value = 'https://api.openai.com/v1/' # 'http://10.96.196.63:11434/v1/'
        'OpenAI API Base', value = 'http://10.96.196.63:11434/v1/'
    )
    temperature = st.slider(
        'Temperature', 0.0, 1.0, value = 0.6, step = 0.1
    )
    if st.sidebar.button('Clear Chat History'):
        st.session_state.clear()
        st.session_state["messages"] = []
        st.rerun()
    st.markdown('---')
    uploaded_file = st.file_uploader("ğŸ“‚ Upload Files", type=["doc", "docx", "txt", "md", "pdf"])


# åˆå§‹åŒ–èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "source_documents" not in st.session_state:
    st.session_state["source_documents"] = []


# é¡¯ç¤ºæ­·å²èŠå¤©è¨˜éŒ„
for msg, src_docs in zip(st.session_state.messages, st.session_state.source_documents):
    if isinstance(msg, HumanMessage):
        with st.chat_message("Human"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("AI"):
            st.markdown(msg.content)
    with st.expander("Knowledge Base References"):
        for doc in src_docs:
            st.markdown(doc)
            st.divider()



def get_llm(model, openai_api_key, openai_api_base, temperature):
    print(f'ä½¿ç”¨ {model}')
    if model == "gpt-3.5-turbo":
        return ChatOpenAI(
            model=model,
            api_key=OPENAI_API_KEY,
            temperature=temperature,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            streaming=True,
        )
    elif model == "llama3.1":
        return ChatOpenAI(
            model=model,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            temperature=temperature,
            streaming=True,
        )


def get_response(user_query, formatted_context, chat_history):
    template = '''
        ä½ æ˜¯ä¸€ä½åœ¨ WiFi 6ã€WiFi 7 èˆ‡ 802.11 å”è­°çš„å°ˆå®¶ï¼Œè«‹æ ¹æ“šåƒè€ƒè³‡è¨Šèˆ‡å°è©±ç´€éŒ„å›ç­”å•é¡Œï¼š

        User question: {user_query}
        çŸ¥è­˜åº«æ“·å–çš„åƒè€ƒè³‡è¨Šï¼š{formatted_context}
        Chat history: {chat_history}

        æ³¨æ„ï¼š
            1. ä½ éœ€è¦ä¾æ“šæä¾›çš„è³‡è¨Šèˆ‡èŠå¤©ç´€éŒ„å›ç­”ï¼Œè«‹å‹¿ç·¨é€ å…§å®¹ã€‚
            2. è‹¥ç„¡è¶³å¤ è³‡è¨Šï¼Œè«‹å›ç­”ã€Œæ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”ã€ã€‚
            3. è«‹ä»¥å°ˆæ¥­ã€ç²¾ç¢ºçš„æ–¹å¼ï¼Œä»¥ç¹é«”ä¸­æ–‡ç‚ºä¸»å›ç­”å•é¡Œã€‚
    '''

    # https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html
    prompt = ChatPromptTemplate.from_template(template)
    llm = get_llm(model, openai_api_key, openai_api_base, temperature)
    chain = prompt | llm | StrOutputParser()
    return chain.stream(
        {
            'user_query': user_query,
            'formatted_context': formatted_context,
            'chat_history': chat_history
        }
    )


import time
# # å¢åŠ è¼¸å…¥æ¡†
if user_query := st.chat_input(placeholder="è«‹è¼¸å…¥æå•å…§å®¹"):
    # å¢åŠ ä½¿ç”¨è€…çš„æå•åˆ°èŠå¤©è¨˜éŒ„
    st.session_state.messages.append(HumanMessage(user_query))
    with st.chat_message("Human"):
        st.markdown(user_query)


    with st.spinner("Searching knowledge base..."):
        time.sleep(1.5)
        retrieved_data = retrieve_similar_chunks(user_query, "wifi_knowledge_embedding_bge", top_k=5)
        context_list = list(zip([context[1] for context in retrieved_data], [context[2] for context in retrieved_data]))
        # Get file_name and its remote path
        file_info_list = list(zip([document[0] for document in retrieved_data], [document[3] for document in retrieved_data]))
        context_chunks = [thing[0] for thing in context_list]
        formatted_context = "\n\n".join(context_chunks)
        # formatted_context = "some sample context" # self.generator.search_db(user_query)

    # æª¢æŸ¥ OpenAI API é‡‘é‘°æ˜¯å¦å­˜åœ¨
    if not openai_api_key:
        st.info("è«‹å…ˆè¼¸å…¥ OpenAI API Key")
        st.stop()


    with st.chat_message("AI"):
        ai_response = st.write_stream(get_response(user_query, formatted_context, st.session_state.messages))
    with st.expander('Knowledge Base References'):
        for i, (document_name, original_text, cosine_distance, file_path) in enumerate(retrieved_data):
            st.markdown("**Source:**")
            file_path = f'./downloads/{document_name}'
            
            # ç¢ºä¿æª”æ¡ˆå­˜åœ¨
            try:
                with open(file_path, "rb") as file:
                    # æŒ‰ä¸‹æŒ‰éˆ•æ™‚ï¼Œæ›´æ–° session_state
                    if st.download_button(label=f"ğŸ“¥ {document_name}", data=file, file_name=document_name, key=f"download_{i}"):
                        st.session_state["downloaded_files"][document_name] = True
            except FileNotFoundError:
                st.warning(f"æª”æ¡ˆ {document_name} ä¸å­˜åœ¨")

            # Content æ›è¡Œä¸¦åŠ å…¥ Tab ç¸®æ’
            st.markdown("**Content:**  \n" + f"&emsp;&emsp;{original_text}", unsafe_allow_html=True)
            st.write(
                f'**Relavance Scoreï¼š** {100 - round(cosine_distance * 100, 2)}%'
            )
            st.divider()

    st.session_state.messages.append(AIMessage(ai_response))
