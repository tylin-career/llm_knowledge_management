from langchain_openai import ChatOpenAI
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from main import retrieve_similar_chunks
from langchain_core.runnables.history import RunnableWithMessageHistory
import streamlit as st
from langchain.chains.conversation.memory import ConversationSummaryBufferMemory
# from langchain_community.memory import ConversationBufferWindowMemory
from config import LLM_PROVIDER, OPENAI_API_KEY
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage


st.set_page_config(page_title='Streamlit çŸ¥è­˜ç®¡ç†å°è©±ç³»çµ±')
st.title("ğŸ’¬ Chatbot")
st.caption("ğŸš€ ASUS Knowledge Management Simulation Powered by NPSPO")

# åŠ å…¥è‡ªè¨‚ CSSï¼Œè®“ä¸‹æ‹‰é¸å–®å±•é–‹æ™‚æœ‰å‹•ç•«
st.markdown(
    """
    <style>
        /* è®“ selectbox æœ¬èº«æ»‘é¼ æ‡¸åœæ™‚æœ‰ç‰¹æ•ˆ */
        div[data-baseweb="select"] {
            transition: all 0.3s ease-in-out;
        }

        div[data-baseweb="select"]:hover {
            background-color: #f0f0f0 !important;
            border-radius: 8px;
        }

        /* è¨­å®šä¸‹æ‹‰é¸å–®æœ¬é«” */
        div[role="listbox"] {
            animation: fadeIn 0.3s ease-in-out;
        }

        /* å®šç¾©å‹•ç•« */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }
    </style>
    """,
    unsafe_allow_html=True
)


with st.sidebar:
    st.title("Navigation and Settings")
    model = st.selectbox(
        'Model', 'gpt-3.5-turbo'
        # 'Model', ['llama3.1', 'gpt-3.5-turbo']
    )
    openai_api_key = st.text_input(
        'OpenAI API Key', value = OPENAI_API_KEY, type = 'password'
        # 'OpenAI API Key', value = 'ollama', type = 'password'
    )
    openai_api_base = st.text_input(
        'OpenAI API Base', value = 'https://api.openai.com/v1/' # 'http://10.96.196.63:11434/v1/'
        # 'OpenAI API Base', value = 'http://10.96.196.63:11434/v1/'
    )
    temperature = st.slider(
        'Temperature', 0.0, 1.0, value = 0.6, step = 0.1
    )


# åˆå§‹åŒ–èŠå¤©æ­·å²
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# é¡¯ç¤ºæ­·å²èŠå¤©è¨˜éŒ„
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("Human"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("AI"):
            st.markdown(msg.content)



def get_llm(model, openai_api_key, openai_api_base, temperature):
    print(f'ä½¿ç”¨ {model}')
    if model == "gpt-3.5-turbo":
        return ChatOpenAI(
            model=model,
            api_key=OPENAI_API_KEY,
            temperature=temperature,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            streaming=True,
        )
    elif model == "llama3.1":
        return ChatOpenAI(
            model=model,
            openai_api_key=openai_api_key,
            openai_api_base=openai_api_base,
            temperature=temperature,
            streaming=True,
        )


def get_response(user_query, formatted_context, chat_history):
    template = '''
        ä½ æ˜¯ä¸€ä½åœ¨ WiFi 6ã€WiFi 7 èˆ‡ 802.11 å”è­°çš„å°ˆå®¶ï¼Œè«‹æ ¹æ“šåƒè€ƒè³‡è¨Šèˆ‡å°è©±ç´€éŒ„å›ç­”å•é¡Œï¼š

        User question: {user_query}
        åƒè€ƒè³‡è¨Šï¼š{formatted_context}
        Chat history: {chat_history}

        æ³¨æ„ï¼š
            1. ä½ åªèƒ½ä¾æ“šæä¾›çš„è³‡è¨Šå›ç­”ï¼Œè«‹å‹¿ç·¨é€ å…§å®¹ã€‚
            2. è‹¥ç„¡è¶³å¤ è³‡è¨Šï¼Œè«‹å›ç­”ã€Œæ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”ã€ã€‚
            3. è«‹ä»¥å°ˆæ¥­ã€ç²¾ç¢ºçš„æ–¹å¼ï¼Œä»¥ç¹é«”ä¸­æ–‡ç‚ºä¸»å›ç­”å•é¡Œã€‚
    '''

    # https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html
    prompt = ChatPromptTemplate.from_template(template)
    llm = get_llm(model, openai_api_key, openai_api_base, temperature)
    chain = prompt | llm | StrOutputParser()
    return chain.stream(
        {
            'user_query': user_query,
            'formatted_context': formatted_context,
            'chat_history': chat_history
        }
    )


# æ¸…é™¤ session ä¸¦é‡è¨­ messages
if st.sidebar.button('æ¸…ç©ºæ­·å²ç´€éŒ„'):
    st.session_state.clear()  # æ¸…é™¤æ‰€æœ‰ session state
    st.session_state["messages"] = []
    st.rerun()


import time
# # å¢åŠ è¼¸å…¥æ¡†
if user_query := st.chat_input(placeholder="è«‹è¼¸å…¥æå•å…§å®¹"):
    # å¢åŠ ä½¿ç”¨è€…çš„æå•åˆ°èŠå¤©è¨˜éŒ„
    st.session_state.messages.append(HumanMessage(user_query))
    with st.chat_message("Human"):
        st.markdown(user_query)


    with st.spinner("Searching knowledge base..."):
        time.sleep(1.5)
        # retrieved_data = retrieve_similar_chunks(user_query, "wifi_knowledge_embedding_bge", top_k=5)
        # context_list = list(zip([context[1] for context in retrieved_data], [context[2] for context in retrieved_data]))
        # # Get file_name and its remote path
        # file_info_list = list(zip([document[0] for document in retrieved_data], [document[3] for document in retrieved_data]))
        # context_chunks = [thing[0] for thing in context_list]
        # formatted_context = "\n\n".join(context_chunks)
        formatted_context = "some sample context" # self.generator.search_db(user_query)

    # æª¢æŸ¥ OpenAI API é‡‘é‘°æ˜¯å¦å­˜åœ¨
    if not openai_api_key:
        st.info("è«‹å…ˆè¼¸å…¥ OpenAI API Key")
        st.stop()


    with st.chat_message("AI"):
        ai_response = st.write_stream(get_response(user_query, formatted_context, st.session_state.messages))
    st.session_state.messages.append(AIMessage(ai_response))


# uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ CSV æª”æ¡ˆ", type="csv")
