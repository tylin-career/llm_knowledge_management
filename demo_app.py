import streamlit as st
from langchain_openai import ChatOpenAI
from config import LLM_PROVIDER, OPENAI_API_KEY
from embedding import get_embedding_model
from postgresql import get_pg_engine
from sqlalchemy import text

# åˆå§‹åŒ– LLM
def get_llm():
    if LLM_PROVIDER == "openai":
        return ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0,
            api_key=OPENAI_API_KEY,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            streaming=True,
        )
    elif LLM_PROVIDER == "ollama":
        return ChatOpenAI(
            model="llama3.1",
            openai_api_key="ollama",
            openai_api_base="http://10.96.196.63:11434/v1/",
            streaming=True,
        )

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_model = get_embedding_model()
# åˆå§‹åŒ– PostgreSQL
pg_engine = get_pg_engine()


def search_relevant_docs(query_text, top_k=3):
    """ åœ¨è³‡æ–™åº«ä¸­æª¢ç´¢èˆ‡ query_text æœ€ç›¸ä¼¼çš„æ–‡ä»¶ """
    query_embedding = embedding_model.embed_query(query_text)

    table_name = "wifi_knowledge_embedding_bge"  # è‹¥ä½¿ç”¨ OpenAIï¼Œæ”¹ç‚º `wifi_knowledge_embedding_openai`
    
    sql = f"""
    SELECT 
        document_name, 
        original_text, 
        file_path, 
        embedding <=> :query_embedding AS similarity
    FROM {table_name}
    ORDER BY similarity ASC
    LIMIT :top_k;
    """

    with pg_engine.connect() as connection:
        result = connection.execute(text(sql), {"query_embedding": query_embedding, "top_k": top_k})
        docs = result.fetchall()

    return docs

def format_context(docs):
    """ æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡æª”å…§å®¹ """
    context = "\n\n".join([f"ğŸ“„ **æª”æ¡ˆ**: {doc[0]}\nğŸ” **å…§å®¹**: {doc[1][:200]}...\nğŸ“‚ **ä¾†æº**: {doc[2]}" for doc in docs])
    return f"### ğŸ” ç›¸é—œæ–‡ä»¶å…§å®¹:\n\n{context}"

def answer_question(query):
    """ é€é RAG æª¢ç´¢è³‡æ–™ä¸¦è®“ LLM å›ç­” """
    docs = search_relevant_docs(query)
    context = format_context(docs)

    messages = [
        ("system", "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜ç®¡ç†åŠ©ç†ï¼Œè«‹æ ¹æ“šæä¾›çš„å…§å®¹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚"),
        ("system", context),
        ("human", query),
    ]

    llm = get_llm()
    response = ""
    for chunk in llm.stream(messages):
        response += chunk.content
        yield chunk.content  # é€æ­¥å›å‚³çµ¦ Streamlit å‰ç«¯










# ---------- Streamlit ä»‹é¢ ----------
st.set_page_config(page_title="çŸ¥è­˜ç®¡ç†ç³»çµ± RAG", page_icon="ğŸ“š", layout="wide")

st.title("ğŸ“š çŸ¥è­˜ç®¡ç†ç³»çµ± (RAG)")
st.write("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼Œç³»çµ±å°‡å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œå…§å®¹ä¸¦å›ç­”æ‚¨çš„å•é¡Œã€‚")

# ä½¿ç”¨è€…è¼¸å…¥
user_input = st.text_area("ğŸ’¬ è¼¸å…¥æ‚¨çš„å•é¡Œ:", height=150)

# é€å‡ºæŸ¥è©¢
if st.button("ğŸ” æŸ¥è©¢"):
    if user_input.strip():
        with st.spinner("ğŸ“– æ­£åœ¨æª¢ç´¢çŸ¥è­˜åº«ä¸¦ç”Ÿæˆå›ç­”..."):
            # é¡¯ç¤ºæª¢ç´¢çµæœ
            docs = search_relevant_docs(user_input)
            if docs:
                st.markdown(format_context(docs))
            else:
                st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„çŸ¥è­˜ã€‚")

            # ç”Ÿæˆç­”æ¡ˆ
            st.markdown("### ğŸ¤– AI å›ç­”:")
            response_area = st.empty()  # å»ºç«‹ä¸€å€‹å€åŸŸä¾†é¡¯ç¤ºå›æ‡‰
            response_text = ""

            for chunk in answer_question(user_input):
                response_text += chunk
                response_area.markdown(response_text)
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥å•é¡Œå¾Œå†æŸ¥è©¢ã€‚")




'''
import chromadb
from sentence_transformers import SentenceTransformer
import openai  # å¦‚æœä½ è¦ä½¿ç”¨ OpenAI API
import ollama  # å¦‚æœä½ è¦ç”¨æœ¬æ©Ÿ Ollamaï¼ˆæ”¯æ´ LLaMA 3.1ï¼‰

# 1ï¸âƒ£ åˆå§‹åŒ– ChromaDB å®¢æˆ¶ç«¯
chroma_client = chromadb.PersistentClient(path="./chroma_langchain_db")
collection = chroma_client.get_or_create_collection(name="documents")

# 2ï¸âƒ£ åˆå§‹åŒ–å‘é‡æ¨¡å‹ï¼ˆä½¿ç”¨ `sentence-transformers`ï¼‰
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # è¼•é‡ç´šä½†æº–ç¢º

# 3ï¸âƒ£ å®šç¾© RAG æŸ¥è©¢å‡½å¼
def query_rag(user_input):
    # ğŸ‘‰ å–å¾—ä½¿ç”¨è€…è¼¸å…¥çš„ embedding
    query_embedding = embedding_model.encode(user_input).tolist()

    # ğŸ‘‰ ç”¨å‘é‡æœå°‹ ChromaDBï¼ˆå–æœ€ç›¸é—œçš„ 3 ç­†è³‡æ–™ï¼‰
    search_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )

    # ğŸ‘‰ çµ„åˆæª¢ç´¢åˆ°çš„å…§å®¹
    retrieved_texts = [doc[0] if isinstance(doc, list) else doc for doc in search_results["documents"]]
    context = "\n".join(retrieved_texts)

    # ğŸ”¹ ä½¿ç”¨ OpenAI APIï¼ˆå¦‚æœä½ æœ‰ API Keyï¼‰
    response = openai.ChatCompletion.create(
        model="gpt-4",  # æˆ– "gpt-3.5-turbo"
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜åº«åŠ©æ‰‹ï¼Œè«‹ä½¿ç”¨æä¾›çš„æ–‡ä»¶ä¾†å›ç­”å•é¡Œã€‚"},
            {"role": "user", "content": f"æ–‡ä»¶å…§å®¹ï¼š\n{context}\n\nä½¿ç”¨è€…å•é¡Œï¼š{user_input}"}
        ]
    )

    return response["choices"][0]["message"]["content"]

# 4ï¸âƒ£ æ¸¬è©¦ RAG æŸ¥è©¢
user_question = input("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼š")
answer = query_rag(user_question)
print("\nğŸ’¡ AI å›ç­”ï¼š\n", answer)


'''