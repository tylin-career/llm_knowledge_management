import os
from config import POSTGRES_URL, EMBEDDING_PROVIDER, OPENAI_API_KEY

import paramiko
from langchain_ollama import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


from langchain_core.vectorstores import InMemoryVectorStore
from datetime import datetime
import pytz
# from tabulate import tabulate

tz = pytz.timezone("Asia/Taipei")


'''
# SSH é€£ç·šè³‡è¨Šï¼ˆè«‹å¡«å…¥ä½ çš„é ç«¯ä¼ºæœå™¨è³‡è¨Šï¼‰
SSH_HOST = "10.96.196.74"
SSH_PORT = 22  # é è¨­ç‚º 22
SSH_USER = "biguser"
SSH_PASSWORD = "npspo"  # å»ºè­°ä½¿ç”¨ SSH é‡‘é‘°é©—è­‰ï¼Œé¿å…æ˜æ–‡å¯†ç¢¼
BASE_PATH = "/mnt/nfs_share/pydio/jacky/05_Technical_Knowledge/00_Internal_Training/03_WiFi_Professsional"

# é€£æ¥ PostgreSQL
engine = get_pg_engine()


def ssh_connect():
    """å»ºç«‹ SSH é€£ç·š"""
    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    client.connect(SSH_HOST, port=SSH_PORT, username=SSH_USER, password=SSH_PASSWORD)
    return client

def get_all_files():
    """é€é SSH å–å¾—æ‰€æœ‰ç¬¦åˆæŒ‡å®šæ ¼å¼çš„æª”æ¡ˆï¼ˆæ’é™¤ test ç›¸é—œè³‡æ–™å¤¾ï¼‰ï¼Œä¸¦å›å‚³ document_name, file_path, file_ext"""
    client = ssh_connect()
    cmd = f"find {BASE_PATH} -type f | grep -v test"
    stdin, stdout, stderr = client.exec_command(cmd)
    
    files = stdout.read().decode().splitlines()
    client.close()

    # éæ¿¾å‡ºç¬¦åˆå‰¯æª”åçš„æª”æ¡ˆ
    valid_extensions = (".docx", ".doc", ".pdf", ".txt", ".md")
    filtered_files = [
        {
            "document_name": os.path.basename(f),  # å–å¾—æª”å
            "file_path": f,  # å®Œæ•´è·¯å¾‘
            "file_ext": os.path.splitext(f)[1]  # å‰¯æª”å
        }
        for f in files if f.lower().endswith(valid_extensions)
    ]

    return filtered_files

def download_file_via_ssh(remote_path):
    """
    é€é SFTP ä¸‹è¼‰é ç«¯æª”æ¡ˆåˆ°æœ¬æ©Ÿ
    :param remote_path: é ç«¯æª”æ¡ˆå®Œæ•´è·¯å¾‘
    :return: æœ¬æ©Ÿæª”æ¡ˆå­˜æ”¾ä½ç½®
    """

    LOCAL_DOWNLOAD_DIR = "./downloads"
    # ç¢ºä¿æœ¬æ©Ÿä¸‹è¼‰è³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists(LOCAL_DOWNLOAD_DIR):
        os.makedirs(LOCAL_DOWNLOAD_DIR)

    # å–å¾—é ç«¯æª”æ¡ˆåç¨±
    file_name = os.path.basename(remote_path)
    local_path = os.path.join(LOCAL_DOWNLOAD_DIR, file_name)

    try:
        client = ssh_connect()
        sftp = client.open_sftp()
        
        print(f"æ­£åœ¨ä¸‹è¼‰: {remote_path} -> {local_path}")
        sftp.get(remote_path, local_path)  # åŸ·è¡Œæª”æ¡ˆä¸‹è¼‰
        print(f"ä¸‹è¼‰å®Œæˆ: {local_path}")
        
        sftp.close()
        client.close()
        
        return local_path  # å›å‚³æœ¬æ©Ÿæª”æ¡ˆè·¯å¾‘
    except Exception as e:
        print(f"ä¸‹è¼‰å¤±æ•— {remote_path}: {e}")
        return None
'''
def get_embedding_model(provider):
    """
    æ ¹æ“š provider åƒæ•¸é¸æ“‡è¦ä½¿ç”¨çš„ embedding æ¨¡å‹ã€‚
    é è¨­ä½¿ç”¨ Ollamaï¼Œä½†å¯ä»¥é€éç’°å¢ƒè®Šæ•¸æˆ–åƒæ•¸åˆ‡æ›æˆ OpenAIã€‚
    """
    if provider == "openai":
        return OpenAIEmbeddings(api_key=OPENAI_API_KEY, model="text-embedding-3-small")  # ä½ å¯ä»¥æ›æˆå…¶ä»– OpenAI embedding æ¨¡å‹
    else:
        ollama_embedding_model = 'quentinz/bge-large-zh-v1.5:latest' # 'bge-m3:latest'
        return OllamaEmbeddings(model=ollama_embedding_model, base_url="http://10.96.196.63:11434")  # ä½ å¯ä»¥æ›æˆä½ åœ¨ Ollama å…§éƒ¨è¨“ç·´çš„ embedding æ¨¡å‹



def get_loader(local_file_path, file_ext:str):
    if file_ext.lower() in ('.doc','.docx'):
        return Docx2txtLoader(local_file_path)
    elif file_ext == '.txt':
        return None

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pandas as pd

def filter_chunks(chunks, percentile_threshold=30):
    """
    æ ¹æ“š TF-IDF è¨ˆç®— chunk çš„é‡è¦æ€§ï¼Œä¸¦éæ¿¾ä½åƒ¹å€¼ chunkã€‚
    
    åƒæ•¸ï¼š
    - chunks: list[str]ï¼Œè¦è©•ä¼°çš„æ–‡æœ¬ç‰‡æ®µ
    - percentile_threshold: intï¼Œè¨­å®šå¤šå°‘ç™¾åˆ†ä½ä»¥ä¸‹çš„ TF-IDF åˆ†æ•¸è¦éæ¿¾
    
    å›å‚³ï¼š
    - DataFrameï¼ŒåŒ…å« chunkã€TF-IDF åˆ†æ•¸ã€æ˜¯å¦ä¿ç•™
    """
    # è¨ˆç®— TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(chunks)
    
    # è¨ˆç®—æ¯å€‹ chunk çš„å¹³å‡ TF-IDF åˆ†æ•¸
    tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=1)

    # è¨­å®šéæ¿¾é–¾å€¼ï¼ˆä½æ–¼æ­¤é–¾å€¼çš„ chunk æœƒè¢«éæ¿¾ï¼‰
    threshold = np.percentile(tfidf_scores, percentile_threshold)

    # åˆ¤æ–·å“ªäº› chunk è¦ä¿ç•™
    filtered_chunks = [(chunk, score, score >= threshold) for chunk, score in zip(chunks, tfidf_scores)]
    
    # è½‰ç‚º DataFrame æ–¹ä¾¿æŸ¥çœ‹
    df = pd.DataFrame(filtered_chunks, columns=["Chunk", "TF-IDF Score", "Keep"])
    # print(tabulate(df, headers="keys", tablefmt="fancy_grid"))
    return df


def get_vector_store(embedding_model, which_db:str):
    if which_db.lower() == 'postgresql':
        return PGVector(
                collection_name="example_collection",
                embedding_function=embedding_model,
                persist_directory="./postgresql_langchain_db",  # Where to save data locally, remove if not necessary
            )
    if which_db.lower() == 'chroma':
        return Chroma(
                collection_name="example_collection",
                embedding_function=embedding_model,
                persist_directory="./chroma_langchain_db",  # Where to save data locally, remove if not necessary
            )
    else:
        return InMemoryVectorStore(embedding_model)



# print(f'å…±æœ‰ {len(get_all_files())}å€‹æª”æ¡ˆ')
# files = get_all_files()[0:1]

data = []
files = [{
    'document_name':'testing_file.docx',
    'file_path':'./downloads/testing_file.docx',
    'file_ext':'.docx'
}]
for file in files:
    current_time = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
    document_name = file['document_name']
    file_path = file['file_path']
    file_ext = file['file_ext']

    # download_file_via_ssh(file['file_path'])

    loader = get_loader(file_path, file_ext)

    document_text = loader.load()
    document_text_page_content = document_text[0].page_content

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=256,
        chunk_overlap=80,
        length_function=len,
        is_separator_regex=False,
        separators=[
            "\n\n",
            "\n",
            " ",
            ".",
            "ã€‚",
            ",",
            "\u200b",  # Zero-width space
            "\uff0c",  # Fullwidth comma
            "\u3001",  # Ideographic comma
            "\uff0e",  # Fullwidth full stop
            "\u3002",  # Ideographic full stop
            "",
        ],
    )
    splitted_chunks = text_splitter.split_text(document_text_page_content)
    df = filter_chunks(splitted_chunks, percentile_threshold=30)
    
    embedding_model = get_embedding_model(EMBEDDING_PROVIDER)

    vectors = embedding_model.embed_documents(splitted_chunks)


    # print(len(vectors))
    # vector_store = get_vector_store(embedding_model, which_db='chroma')


    
    for idx, (chunk, vector) in enumerate(zip(splitted_chunks, vectors), start=1):
        data.append([idx, document_name, chunk, '', vector, current_time, file_path, file])
        # print(idx, document_name, chunk, '', vector, current_time, file_path, file)
        # print('-------------------------------------------')



df = pd.DataFrame(data, columns=['id', 'document_name', 'original_text', 'cleaned_text', 'embedding', 'process_datetime', 'file_path', 'metadata'])

# from IPython.display import display
# display(df)




from sqlalchemy import create_engine
import chromadb

# 1. å•Ÿå‹• ChromaDBï¼ˆæŒä¹…åŒ–æ¨¡å¼ï¼‰
chroma_client = chromadb.PersistentClient(path="./chroma_langchain_db")

# 2. å‰µå»ºæˆ–ç²å– Collection
collection = chroma_client.get_or_create_collection(name="example_collection")

# 3. æ‰¹é‡æ’å…¥ DataFrameï¼ˆBulk Insertï¼‰
collection.add(
    ids=df["id"].astype(str).tolist(),  # ChromaDB çš„ ID éœ€è¦æ˜¯å­—ä¸²
    documents=df["original_text"].astype(str).tolist(),  # åŸå§‹æ–‡æœ¬
    metadatas=df.apply(lambda row: {  # Metadata æ˜¯ JSON æ ¼å¼
        "document_name": row["document_name"],
        "file_path": row["file_path"],
        "file_ext": row["metadata"]['file_ext'],
        "cleaned_text": row["cleaned_text"],
        "process_datetime": str(row["process_datetime"])  # ç¢ºä¿æ˜¯å­—ä¸²
    }, axis=1).tolist(),
    embeddings=df["embedding"].tolist()  # å‘é‡åµŒå…¥ï¼Œå¿…é ˆæ˜¯ List[List[float]]
)







import chromadb
from sentence_transformers import SentenceTransformer
import openai  # å¦‚æœä½ è¦ä½¿ç”¨ OpenAI API
import ollama  # å¦‚æœä½ è¦ç”¨æœ¬æ©Ÿ Ollamaï¼ˆæ”¯æ´ LLaMA 3.1ï¼‰

# 1ï¸âƒ£ åˆå§‹åŒ– ChromaDB å®¢æˆ¶ç«¯
chroma_client = chromadb.PersistentClient(path="./chroma_langchain_db")
collection = chroma_client.get_or_create_collection(name="documents")

# 2ï¸âƒ£ åˆå§‹åŒ–å‘é‡æ¨¡å‹ï¼ˆä½¿ç”¨ `sentence-transformers`ï¼‰
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # è¼•é‡ç´šä½†æº–ç¢º

# 3ï¸âƒ£ å®šç¾© RAG æŸ¥è©¢å‡½å¼
def query_rag(user_input):
    # ğŸ‘‰ å–å¾—ä½¿ç”¨è€…è¼¸å…¥çš„ embedding
    query_embedding = embedding_model.encode(user_input).tolist()

    # ğŸ‘‰ ç”¨å‘é‡æœå°‹ ChromaDBï¼ˆå–æœ€ç›¸é—œçš„ 3 ç­†è³‡æ–™ï¼‰
    search_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )

    # ğŸ‘‰ çµ„åˆæª¢ç´¢åˆ°çš„å…§å®¹
    retrieved_texts = [doc[0] if isinstance(doc, list) else doc for doc in search_results["documents"]]
    context = "\n".join(retrieved_texts)

    # ğŸ”¹ ä½¿ç”¨ OpenAI APIï¼ˆå¦‚æœä½ æœ‰ API Keyï¼‰
    response = openai.ChatCompletion.create(
        model="gpt-4",  # æˆ– "gpt-3.5-turbo"
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜åº«åŠ©æ‰‹ï¼Œè«‹ä½¿ç”¨æä¾›çš„æ–‡ä»¶ä¾†å›ç­”å•é¡Œã€‚"},
            {"role": "user", "content": f"æ–‡ä»¶å…§å®¹ï¼š\n{context}\n\nä½¿ç”¨è€…å•é¡Œï¼š{user_input}"}
        ]
    )

    return response["choices"][0]["message"]["content"]

# 4ï¸âƒ£ æ¸¬è©¦ RAG æŸ¥è©¢
user_question = input("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼š")
answer = query_rag(user_question)
print("\nğŸ’¡ AI å›ç­”ï¼š\n", answer)